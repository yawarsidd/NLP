# -*- coding: utf-8 -*-
"""TF-IDF vectorizer and Word2vec .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14gazdF1EfMVeLJwHAOmI2h0O6n67krnI

Reading E-mails data whether it is a Ham or Spam
"""

import pandas as pd
import numpy as np

sms = pd.read_csv('sms.csv')

sms.shape

sms.head(20)

sms.label.value_counts()/sum(sms.label.value_counts()) * 100

# convert label to a numeric variable
sms['label'] = sms.label.map({'ham':0, 'spam':1})

sms.head()

import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
import textblob
from textblob import TextBlob
from wordcloud import WordCloud,STOPWORDS
nltk.download('wordnet')

import re
word=WordNetLemmatizer()

def pre_process_text(x):
    x=x.lower()
    x=x.strip()
    x = re.sub(r' +', ' ', x)
    x = re.sub(r"[-()\"#/@;:{}`+=~|.!?,^_^'0-9]", "", x)
    x=x=" ".join([word.lemmatize(w,pos='v') for w in x.split()])  # lemmatization
    #x=str(TextBlob(x).correct()) # spelling correction
    return(x)

sms['message']=sms['message'].apply(lambda x:pre_process_text(x))

sms.head()

x=sms.message
y=sms.label

from sklearn.model_selection import train_test_split

train_x,test_x,train_y,test_y= train_test_split(x,y,test_size=0.3,random_state=100)
print(train_x.shape)
print(test_x.shape)
print(test_y.shape)
print(train_y.shape)

"""Vectorizing the E-mails"""

vect = CountVectorizer(analyzer='word',lowercase=True,ngram_range=(1,2), max_features=700, max_df=1.0, min_df=10,stop_words=['xxx','lui','ll','ltdecimal it','re'])

vect.fit(train_x)

X_train_dtm = vect.transform(train_x)
X_train_dtm.todense()

vect.get_feature_names_out()

DTM=pd.DataFrame(X_train_dtm.todense(),columns=vect.get_feature_names_out())

DTM

# Commented out IPython magic to ensure Python compatibility.
# Wordcloud using frequency of words
# using actual text
from matplotlib import pyplot as plt
wordcloud = WordCloud(stopwords=STOPWORDS).generate(' '.join(train_x.tolist()))

# %matplotlib inline
fig = plt.figure(figsize=(50,50))
plt.imshow(wordcloud)

"""Building a Naive Bayes model"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(DTM,train_y)

X_test_dtm = vect.transform(test_x)  # convert x_test to dtm

# make class predictions for X_test_dtm
y_pred_class_train = nb.predict(DTM)
y_pred_class = nb.predict(X_test_dtm.toarray())

# calculate accuracy of class predictions
from sklearn import metrics
print(metrics.accuracy_score(test_y, y_pred_class))

# confusion matrix
print(metrics.confusion_matrix(test_y, y_pred_class))

print(metrics.classification_report(train_y, y_pred_class_train))

print(metrics.classification_report(test_y, y_pred_class))

"""Using KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {'n_neighbors': [4,5,6], 'weights': ['distance','uniform']}

model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring = 'precision')
model = model.fit(DTM, train_y)

model.best_params_

knn= KNeighborsClassifier(n_neighbors=4,weights='uniform')

knn.fit(DTM,train_y)

y_pred_class_train = model.predict(DTM)
y_pred_class = model.predict(X_test_dtm.toarray())

# calculate accuracy of class predictions
from sklearn import metrics
print(metrics.accuracy_score(test_y, y_pred_class))

# confusion matrix
print(metrics.confusion_matrix(test_y, y_pred_class))

print(metrics.classification_report(train_y, y_pred_class_train))

print(metrics.classification_report(test_y, y_pred_class))

"""The accuracy on test data using Naive_bayes is 97.54% and KNN is 93%."""

# Word2vec

! pip install gensim

import gensim.downloader as api
wv=api.load("word2vec-google-news-300")  # a pre trained word2vec model called google-news-300

vect_king=wv['king']   # a 300 size vector can be generate using google news 300
vect_king

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()

# Initialize Porter Stemmer
ps = PorterStemmer()

corpus = []
for i in range(len(sms)):
    # Remove non-alphabetic characters and convert to lowercase
    review = re.sub('[^a-zA-Z]', ' ', sms['message'][i])
    review = review.lower()
    # Tokenize the review
    review = review.split()
    # Stemming and removing stopwords
    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]
    # Join the words back into a single string
    review = ' '.join(review)
    corpus.append(review)

len(corpus)

corpus

from nltk import sent_tokenize
from gensim.utils import simple_preprocess
nltk.download('punkt')

corpus

len(corpus)

tokenized_data = [text.split() for text in corpus]

len(tokenized_data)

tokenized_data

def total_distinct_word_length(tokenized_data):
    # Flatten the list of lists into a single list
    flattened_tokens = [word for sublist in tokenized_data for word in sublist]

    # Create a set from the flattened list to get unique words
    unique_words = set(flattened_tokens)

    # Return the length of the set of unique words
    return len(unique_words)

# Example usage:
#tokenized_data = [["apple", "banana"], ["apple", "orange", "grape"], ["banana"]]
total_length = total_distinct_word_length(tokenized_data)
print(total_length)

import gensim

model = gensim.models.Word2Vec(tokenized_data,window=5,min_count=1)   # the word must be present two times in whole doc to enter in vocab

model.wv.index_to_key      # all the selected vocabulary for training distinct words

model.wv.similar_by_word('slave')

word_embeddings = {}

# Iterate over each word in the vocabulary
for word in model.wv.index_to_key:
    word_embeddings[word] = model.wv[word]

word_embeddings

pd.DataFrame(word_embeddings)

# avgWord2vec sentences embeddings then use ml models

# Convert each text to a fixed-length vector by averaging word vectors
X = []
for text in tokenized_data:
    vectors = [model.wv[word] for word in text if word in model.wv]   # first we take a single word from each tokenize sentence
    if vectors:  # Check if the list is not empty                     # then if that word is present in word2vec vocublary then put that word in that list
      X.append(np.mean(vectors, axis=0))                              # after that caclculate mean of all words present in that sentence. If no words
                                                                      # is present in a vacubolary of word2vec of a particular sentence then put a zero size vector
    else:  # If list is empty, append a placeholder vector
        X.append(np.zeros(model.vector_size))  # Use zeros as a placeholder

X = np.array(X)
y = np.array(sms['label'])

#Iterating over each word in the text, checking if it exists in the Word2Vec model's vocabulary.
#Creating a list vectors containing the word vectors of the words found in the vocabulary.
#Checking if the list vectors is not empty.
#If the list is not empty, calculating the mean of the word vectors along the first axis (axis=0).

#Suppose 1st sentence
#'this': [0.1, 0.2, 0.3]    Mean: 0.1+0.2+0.3+0.4/4=0.25  1st sentence= [0.25,0.35,0.45]   Sentence embedding for 1st messsage
#'is': [0.2, 0.3, 0.4]      Mean: 0.2+0.3+0.4+0.5/4=0.35
#'a': [0.3, 0.4, 0.5]       Mean: 0.3+0.4+0.5+0.6/4=0.45
#'sentence': [0.4, 0.5, 0.6]

X.shape

y.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a logistic regression classifier
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

param_grid = {'n_neighbors': [4,5,6,8,10,12,14], 'weights': ['distance','uniform']}
model = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, scoring = 'precision')
model = model.fit(X_train, y_train)
model.best_params_

knn_W= KNeighborsClassifier(n_neighbors=14,weights='distance')
knn_W.fit(X_train,y_train)

# Predict on the test set
y_pred =knn_W.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB  # dom't take negative values MNB
gensigb = GaussianNB()
gensigb.fit(X_train,y_train)

# Predict the labels of test data
y_pred = gensigb.predict(X_test)

# Calculate accuracy
accuracy = metrics.accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# MNB is good with dealing count values or text data where we convert to count or frequencies if neg values are there it not work.
# Basically MNB is used for classification task with discrete features
# GNB is good for classification task when features are continuous. It assumes features follows gaussian or normal distribution
# Bernaulli NB is similar to Multinomial Naive Bayes but is designed for binary/boolean features. It's commonly used in text classification tasks where features are binary indicators